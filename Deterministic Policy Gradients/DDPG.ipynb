{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add environment to PYTHONPATH\n",
    "import sys\n",
    "import os\n",
    "env_path = os.path.join(os.path.abspath(os.getcwd()), '..\\\\Environments\\\\ContinuousCartPole')\n",
    "sys.path.append(env_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from continuous_cartpole import ContinuousCartPoleEnv\n",
    "\n",
    "import gym\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discount factor\n",
    "γ = 0.99\n",
    "\n",
    "#soft target constant\n",
    "τ = 0.001\n",
    "\n",
    "#Learning rates\n",
    "α_θ = 0.0001\n",
    "αw = 0.001\n",
    "\n",
    "#episode to run\n",
    "NUM_EPISODES = 1000\n",
    "\n",
    "#steps per episode\n",
    "MAX_STEPS = 5000\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "#replay buffer\n",
    "BUFFER_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(DeterministicPolicy, self).__init__()\n",
    "        self.input_batch_norm = nn.BatchNorm1d(obs_space)\n",
    "        \n",
    "        self.first_layer = nn.Linear(obs_space, 400)\n",
    "        self.first_batch_norm = nn.BatchNorm1d(400)\n",
    "        \n",
    "        self.second_layer = nn.Linear(400, 300)\n",
    "        self.second_batch_norm = nn.BatchNorm1d(300)\n",
    "        \n",
    "        self.output_layer = nn.Linear(300, action_space)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_batch_norm(x)\n",
    "        \n",
    "        x = self.first_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.first_batch_norm(x)\n",
    "        \n",
    "        x = self.second_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.second_batch_norm(x)\n",
    "        \n",
    "        output = self.output_layer(x)\n",
    "        actions = torch.tanh(output)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.input_batch_norm = nn.BatchNorm1d(obs_space)\n",
    "        \n",
    "        self.first_layer = nn.Linear(obs_space, 400)\n",
    "        self.first_batch_norm = nn.BatchNorm1d(400)\n",
    "        \n",
    "        self.second_layer = nn.Linear(400 + action_space, 300)\n",
    "        \n",
    "        self.output_layer = nn.Linear(300, 1)\n",
    "        \n",
    "    def forward(self, x, actions):\n",
    "        x = self.input_batch_norm(x)\n",
    "        \n",
    "        x = self.first_layer(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.first_batch_norm(x)\n",
    "        \n",
    "        x_with_action = torch.cat([x, actions], dim=1)\n",
    "        \n",
    "        x = self.second_layer(x_with_action)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        q_val = self.output_layer(x)\n",
    "        \n",
    "        return q_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size=1, mu=0, theta=0.05, sigma=0.25):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy, state):\n",
    "    \n",
    "    state_tensor = torch.from_numpy(state).float().unsqueeze(0) \n",
    "    \n",
    "    action = policy(state_tensor)\n",
    "    action = action + ou.sample().item()\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(batch, policy_network, q_network, target_policy_network, target_q_network, policy_optimizer, q_optimizer, τ):\n",
    "    \n",
    "    #fix this. I'm creating a new tensor here from action so it doesnt back propagate back to policy function\n",
    "    action_batch = torch.stack([item[1] for item in batch]).squeeze(1)\n",
    "    state_batch = torch.Tensor([item[0] for item in batch])\n",
    "    new_state_batch = torch.Tensor([item[3] for item in batch])\n",
    "    rewards_batch = [item[2] for item in batch]\n",
    "    \n",
    "    train([policy_network, q_network, target_policy_network, target_q_network])\n",
    "    \n",
    "    policy_action_batch = target_policy_network(new_state_batch)\n",
    "    target_q_batch = target_q_network(new_state_batch, policy_action_batch)\n",
    "    target_q_batch = torch.Tensor([reward + γ * q for reward, q in zip(rewards_batch, target_q_batch)]).unsqueeze(1)\n",
    "    q_batch = q_network(state_batch, action_batch)\n",
    "    \n",
    "    critic_loss = F.mse_loss(target_q_batch, q_batch)\n",
    "    actor_loss = -torch.mean(q_batch)\n",
    "    \n",
    "    q_optimizer.zero_grad()\n",
    "    critic_loss.backward(retain_graph = True)\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    \n",
    "    q_optimizer.step()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    update_target_net(policy_network, target_policy_network, τ)\n",
    "    update_target_net(q_network, target_q_network, τ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(models):\n",
    "    for model in models:\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(models):\n",
    "    for model in models:\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_net(net, target_net, τ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            new_param = τ * param.data + (1 - τ) * target_param.data\n",
    "            target_param.data.copy_(new_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init environment\n",
    "#env = gym.make('LunarLanderContinuous-v2').env\n",
    "#env = gym.make('MountainCarContinuous-v0').env\n",
    "env = ContinuousCartPoleEnv()\n",
    "\n",
    "#env parameters\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "\n",
    "#set seeds\n",
    "# np.random.seed(1)\n",
    "# random.seed(1)\n",
    "# env.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "#init networks\n",
    "policy_network = DeterministicPolicy(obs_space, action_space)\n",
    "q_network = QNetwork(obs_space, action_space)\n",
    "\n",
    "target_policy_network = DeterministicPolicy(obs_space, action_space)\n",
    "target_q_network = QNetwork(obs_space, action_space)\n",
    "\n",
    "#target network same weights\n",
    "for param, target_param in zip(policy_network.parameters(), target_policy_network.parameters()):\n",
    "    param.data.copy_(target_param.data)\n",
    "    \n",
    "for param, target_param in zip(q_network.parameters(), target_q_network.parameters()):\n",
    "    param.data.copy_(target_param.data)\n",
    "\n",
    "\n",
    "#init optimizers\n",
    "policy_optimizer = optim.Adam(policy_network.parameters(), lr=α_θ)\n",
    "q_optimizer = optim.Adam(q_network.parameters(), lr=αw, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e2e06f48d64f6782396c270c566dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [300, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-75f33872dfd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtotal_steps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2000\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_policy_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_q_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mτ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-eb10ff0a7942>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[1;34m(batch, policy_network, q_network, target_policy_network, target_q_network, policy_optimizer, q_optimizer, τ)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mq_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mcritic_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mpolicy_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [300, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "total_steps = 0\n",
    "\n",
    "for episode in tqdm_notebook(range(NUM_EPISODES)):\n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    ou = OUNoise()\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        #env.render()\n",
    "        total_steps += 1\n",
    "        \n",
    "        eval([policy_network])\n",
    "        action = select_action(policy_network, state)\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action.squeeze(0).detach().numpy())\n",
    "        score += reward\n",
    "        \n",
    "        #reward += 100*((np.sin(3*new_state[0]) * 0.0025 + 0.5 * new_state[1] * new_state[1]) - (np.sin(3*state[0]) * 0.0025 + 0.5 * state[1] * state[1]))\n",
    "        \n",
    "        replay_buffer.append([state, action, reward, new_state])\n",
    "        \n",
    "        if len(replay_buffer) >= BATCH_SIZE and total_steps > 2000:\n",
    "            batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "            update_parameters(batch, policy_network, q_network, target_policy_network, target_q_network, policy_optimizer, q_optimizer, τ)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "    scores.append(score)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-baa45cf6ac7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'grey'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Training score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'blue'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Target Policy score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'episodes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy_scores' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set()\n",
    "\n",
    "plt.plot(scores, color='grey', label='Training score')\n",
    "plt.plot(policy_scores, color='blue', label='Target Policy score')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('episodes')\n",
    "plt.title('Score history of MountainCarContinuous with DDPG')\n",
    "plt.legend()\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(policy_scores)).reshape(-1, 1), np.array(policy_scores).reshape(-1, 1))\n",
    "y_pred = reg.predict(np.arange(len(policy_scores)).reshape(-1, 1))\n",
    "plt.plot(y_pred, color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    for step in range(MAX_STEPS):\n",
    "        env.render()\n",
    "\n",
    "        new_state, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c3174eafd04ad09dee3d76f5858076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "testing_scores = []\n",
    "\n",
    "for _ in tqdm_notebook(range(5)):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    for step in range(MAX_STEPS):\n",
    "        #env.render()\n",
    "        eval([policy_network])\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action = policy_network(state_tensor)\n",
    "        new_state, reward, done, info = env.step([action.item()])\n",
    "        \n",
    "        score += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    testing_scores.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.0, 13.0, 18.0, 18.0, 16.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0014359440480486948"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(testing_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0626758033809403e-13"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(testing_scores).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
